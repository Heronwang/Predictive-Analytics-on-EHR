{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>Stage_Progress</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>SBP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>ldl</th>\n",
       "      <th>HGB</th>\n",
       "      <th>heart</th>\n",
       "      <th>daily_dosage</th>\n",
       "      <th>duration</th>\n",
       "      <th>Female</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6823.000000</td>\n",
       "      <td>6823.000000</td>\n",
       "      <td>6823.000000</td>\n",
       "      <td>1481.000000</td>\n",
       "      <td>1598.000000</td>\n",
       "      <td>1855.000000</td>\n",
       "      <td>1867.000000</td>\n",
       "      <td>1301.000000</td>\n",
       "      <td>2061.000000</td>\n",
       "      <td>2181.000000</td>\n",
       "      <td>2181.000000</td>\n",
       "      <td>2181.000000</td>\n",
       "      <td>6823.000000</td>\n",
       "      <td>6823.000000</td>\n",
       "      <td>6823.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>146.601055</td>\n",
       "      <td>412.961894</td>\n",
       "      <td>0.356002</td>\n",
       "      <td>1.327076</td>\n",
       "      <td>6.720413</td>\n",
       "      <td>134.324755</td>\n",
       "      <td>79.569855</td>\n",
       "      <td>87.247932</td>\n",
       "      <td>13.832693</td>\n",
       "      <td>0.774415</td>\n",
       "      <td>276.653141</td>\n",
       "      <td>79.741862</td>\n",
       "      <td>0.554741</td>\n",
       "      <td>70.626997</td>\n",
       "      <td>3.385314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>87.964163</td>\n",
       "      <td>307.245821</td>\n",
       "      <td>0.478851</td>\n",
       "      <td>0.355466</td>\n",
       "      <td>1.617688</td>\n",
       "      <td>14.753248</td>\n",
       "      <td>10.176755</td>\n",
       "      <td>28.357415</td>\n",
       "      <td>1.642781</td>\n",
       "      <td>0.418063</td>\n",
       "      <td>478.411077</td>\n",
       "      <td>58.766049</td>\n",
       "      <td>0.497031</td>\n",
       "      <td>9.145888</td>\n",
       "      <td>1.221210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-78.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>2.890000</td>\n",
       "      <td>91.990000</td>\n",
       "      <td>44.950000</td>\n",
       "      <td>26.100000</td>\n",
       "      <td>8.820000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>69.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>5.630000</td>\n",
       "      <td>124.745000</td>\n",
       "      <td>72.860000</td>\n",
       "      <td>66.930000</td>\n",
       "      <td>12.680000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>144.000000</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.290000</td>\n",
       "      <td>6.380000</td>\n",
       "      <td>133.680000</td>\n",
       "      <td>79.090000</td>\n",
       "      <td>83.670000</td>\n",
       "      <td>13.930000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>225.000000</td>\n",
       "      <td>571.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>7.450000</td>\n",
       "      <td>143.440000</td>\n",
       "      <td>86.290000</td>\n",
       "      <td>105.090000</td>\n",
       "      <td>14.980000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>299.000000</td>\n",
       "      <td>1429.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.020000</td>\n",
       "      <td>16.610000</td>\n",
       "      <td>211.090000</td>\n",
       "      <td>112.930000</td>\n",
       "      <td>198.590000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2550.000000</td>\n",
       "      <td>540.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id         time  Stage_Progress   creatinine      glucose  \\\n",
       "count  6823.000000  6823.000000     6823.000000  1481.000000  1598.000000   \n",
       "mean    146.601055   412.961894        0.356002     1.327076     6.720413   \n",
       "std      87.964163   307.245821        0.478851     0.355466     1.617688   \n",
       "min       0.000000   -78.000000        0.000000     0.240000     2.890000   \n",
       "25%      69.000000   182.000000        0.000000     1.080000     5.630000   \n",
       "50%     144.000000   374.000000        0.000000     1.290000     6.380000   \n",
       "75%     225.000000   571.000000        1.000000     1.530000     7.450000   \n",
       "max     299.000000  1429.000000        1.000000     3.020000    16.610000   \n",
       "\n",
       "               SBP          DBP          ldl          HGB        heart  \\\n",
       "count  1855.000000  1867.000000  1301.000000  2061.000000  2181.000000   \n",
       "mean    134.324755    79.569855    87.247932    13.832693     0.774415   \n",
       "std      14.753248    10.176755    28.357415     1.642781     0.418063   \n",
       "min      91.990000    44.950000    26.100000     8.820000     0.000000   \n",
       "25%     124.745000    72.860000    66.930000    12.680000     1.000000   \n",
       "50%     133.680000    79.090000    83.670000    13.930000     1.000000   \n",
       "75%     143.440000    86.290000   105.090000    14.980000     1.000000   \n",
       "max     211.090000   112.930000   198.590000    19.000000     1.000000   \n",
       "\n",
       "       daily_dosage     duration       Female          age         race  \n",
       "count   2181.000000  2181.000000  6823.000000  6823.000000  6823.000000  \n",
       "mean     276.653141    79.741862     0.554741    70.626997     3.385314  \n",
       "std      478.411077    58.766049     0.497031     9.145888     1.221210  \n",
       "min        2.000000     1.000000     0.000000    46.000000     0.000000  \n",
       "25%       20.000000    30.000000     0.000000    65.000000     3.000000  \n",
       "50%       50.000000    90.000000     1.000000    72.000000     4.000000  \n",
       "75%      320.000000    90.000000     1.000000    78.000000     4.000000  \n",
       "max     2550.000000   540.000000     1.000000    86.000000     4.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels\n",
    "labels = pd.read_csv(\"dataScienceTask/T_stage.csv\")\n",
    "labels['Stage_Progress'] = labels['Stage_Progress'].astype(np.int32)\n",
    "\n",
    "\n",
    "# lab measurements \n",
    "SBP = pd.read_csv(\"dataScienceTask/T_SBP.csv\")\n",
    "DBP = pd.read_csv(\"dataScienceTask/T_DBP.csv\")\n",
    "ldl = pd.read_csv(\"dataScienceTask/T_ldl.csv\")\n",
    "creatinine = pd.read_csv(\"dataScienceTask/T_creatinine.csv\")\n",
    "HGB = pd.read_csv(\"dataScienceTask/T_HGB.csv\")\n",
    "glucose = pd.read_csv(\"dataScienceTask/T_glucose.csv\")\n",
    "\n",
    "SBP = SBP.rename(columns={'value':\"SBP\"})\n",
    "DBP = DBP.rename(columns={'value':\"DBP\"})\n",
    "ldl = ldl.rename(columns={'value':\"ldl\"})\n",
    "creatinine = creatinine.rename(columns={'value':\"creatinine\"})\n",
    "HGB = HGB.rename(columns={'value':\"HGB\"})\n",
    "glucose = glucose.rename(columns={'value':\"glucose\"})\n",
    "\n",
    "\n",
    "# medication history\n",
    "drug_cat = {} # bookkeeping of heart dieases drug and dieabetes drug\n",
    "drug_cat['diabetes'] = [\"metformin\",\"canagliflozin\",\"dapagliflozin\"]\n",
    "drug_cat['heart'] = [\"atorvastatin\",\"losartan\",\"Simvastatin\",\"metoprolol\",\"valsartan\",\\\n",
    "                     \"atenolol\",\"rosuvastatin\",\"pravastatin\",\"carvedilol\",\"lovastatin\",\\\n",
    "                     \"olmesartan\",\"bisoprolol\",\"propranolol\",\"irbesartan\",\"nebivolol\",\\\n",
    "                     \"telmisartan\",\"labetalol\",\"pitavastatin\",\"simvastatin\"]\n",
    "\n",
    "meds = pd.read_csv(\"dataScienceTask/T_meds.csv\")\n",
    "meds['heart'] = (meds.drug.isin(drug_cat['heart']).astype(np.int32)) # dosage of heart disease medication\n",
    "meds['duration'] = meds.end_day - meds.start_day \n",
    "meds = meds.rename(columns={'start_day':\"time\"})\n",
    "meds = meds[['id','time','heart','daily_dosage','duration']]\n",
    "\n",
    "\n",
    "# demographics \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "demo = pd.read_csv(\"dataScienceTask/T_demo.csv\")\n",
    "demo['Female'] = (demo['gender'] == 'Female').astype(np.int32)\n",
    "demo['race'] = LabelEncoder().fit(demo.race.values).transform(demo.race.values)\n",
    "demo = demo[['id','Female','age','race']]\n",
    "\n",
    "\n",
    "# merge medication history with lab measurements\n",
    "df = meds.copy()\n",
    "rights = [SBP,DBP,ldl,creatinine,HGB,glucose]\n",
    "for right in rights:\n",
    "    df = df.merge(right,on=['id','time'],how='outer')\n",
    "df = df.sort_values(['id','time']).reset_index(drop=True)\n",
    "# merge with labels\n",
    "df = df.merge(labels,on='id',how='left')\n",
    "# merge with demographics \n",
    "df = df.merge(demo,on='id',how='left')\n",
    "## output\n",
    "f_labs,f_meds,f_demo = ['creatinine','glucose','SBP','DBP','ldl','HGB'],['heart','daily_dosage','duration'],['Female','age','race'],\n",
    "f_others = ['id','time','Stage_Progress']\n",
    "\n",
    "df = df[f_others+f_labs+f_meds+f_demo]\n",
    "df.to_csv(\"df.csv\",sep=',', index=False, header=True)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('creatinine', 3.02, 0.23999999999999999)\n",
      "('glucose', 16.609999999999999, 2.8900000000000001)\n",
      "('SBP', 211.09, 91.989999999999995)\n",
      "('DBP', 112.93000000000001, 44.950000000000003)\n",
      "('ldl', 198.59, 26.100000000000001)\n",
      "('HGB', 19.0, 8.8200000000000003)\n",
      "('heart', 1.0, 0.0)\n",
      "('daily_dosage', 2550.0, 2.0)\n",
      "('duration', 540.0, 1.0)\n",
      "('Female', 1.0, 0.0)\n",
      "('age', 86.0, 46.0)\n",
      "('race', 4.0, 0.0)\n",
      "       no_aug           id        time  Stage_Progress   creatinine  \\\n",
      "count  3000.0  3000.000000  3000.00000     3000.000000  3000.000000   \n",
      "mean      0.0   149.500000     4.50000        0.333333     0.192640   \n",
      "std       0.0    86.616497     2.87276        0.471483     0.215164   \n",
      "min       0.0     0.000000     0.00000        0.000000     0.000000   \n",
      "25%       0.0    74.750000     2.00000        0.000000     0.000000   \n",
      "50%       0.0   149.500000     4.50000        0.000000     0.000000   \n",
      "75%       0.0   224.250000     7.00000        1.000000     0.375000   \n",
      "max       0.0   299.000000     9.00000        1.000000     1.000000   \n",
      "\n",
      "           glucose          SBP          DBP          ldl          HGB  \\\n",
      "count  3000.000000  3000.000000  3000.000000  3000.000000  3000.000000   \n",
      "mean      0.148092     0.219633     0.316669     0.153713     0.325211   \n",
      "std       0.163426     0.198265     0.273811     0.206389     0.268498   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.144679     0.252729     0.382245     0.000000     0.375246   \n",
      "75%       0.261662     0.380458     0.540747     0.305221     0.553045   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "             heart  daily_dosage     duration       Female          age  \\\n",
      "count  3000.000000   3000.000000  3000.000000  3000.000000  3000.000000   \n",
      "mean      0.470000      0.062783     0.089458     0.586667     0.609583   \n",
      "std       0.499182      0.153848     0.114226     0.492514     0.230829   \n",
      "min       0.000000      0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000      0.000000     0.000000     0.000000     0.450000   \n",
      "50%       0.000000      0.007064     0.053803     1.000000     0.625000   \n",
      "75%       1.000000      0.018838     0.165121     1.000000     0.800000   \n",
      "max       1.000000      1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "              race  \n",
      "count  3000.000000  \n",
      "mean      0.850000  \n",
      "std       0.300744  \n",
      "min       0.000000  \n",
      "25%       1.000000  \n",
      "50%       1.000000  \n",
      "75%       1.000000  \n",
      "max       1.000000  \n"
     ]
    }
   ],
   "source": [
    "n_step = 10\n",
    "n_aug = 1\n",
    "dropout = 0.\n",
    "noise_ratio = 0.\n",
    "feature_names = f_labs + f_meds + f_demo\n",
    "\n",
    "# ============ optional: relative time step\n",
    "# def delshift_nan(trunc,feature_names,n_step):\n",
    "#     h,w = trunc.shape\n",
    "#     for f in feature_names:\n",
    "#         nonnan = trunc[f].values[~np.isnan(trunc[f].values)]\n",
    "#         trunc.loc[:,f] = np.hstack(([np.nan]*h, nonnan))[-h:]\n",
    "#     return trunc.tail(n_step).values\n",
    "\n",
    "# data = pd.DataFrame(columns=df.columns,data=np.nan*np.zeros((n_step*300,len(df.columns))))\n",
    "# for i,g in df.groupby('id'):\n",
    "#     value = delshift_nan(g,feature_names,n_step)\n",
    "#     if value.shape[0] < n_step:\n",
    "#         value = np.vstack((np.nan*np.zeros((n_step-value.shape[0],value.shape[1])),value))\n",
    "#     data[i*n_step:(i+1)*n_step] = value   \n",
    "\n",
    "# data[f_demo+f_others] = np.array([df[f_demo+f_others].groupby('id').head(1).values[i//n_step] for i in range(data.shape[0])])\n",
    "# data['time'] = np.array(range(n_step)*300)\n",
    "# data.to_csv(\"data_rlt{}.csv\".format(str(n_step)),sep=',', index=False, header=True)\n",
    "# ============ relative time step\n",
    "\n",
    "\n",
    "# # ============ optional: absolute time step\n",
    "# data = pd.DataFrame(columns=df.columns,data=np.nan*np.zeros((300*n_step,len(df.columns))))\n",
    "# for i,g in df.groupby('id'):\n",
    "#     value = g.tail(n_step).values\n",
    "#     if value.shape[0] < n_step:\n",
    "#         value = np.vstack((np.nan*np.zeros((n_step-value.shape[0],value.shape[1])),value))   \n",
    "#     data[i*n_step:(i+1)*n_step] = value\n",
    "    \n",
    "# data[f_demo+f_others] = np.array([df[f_demo+f_others].groupby('id').head(1).values[i//n_step] for i in range(data.shape[0])])   \n",
    "# data['time'] = np.array(range(n_step)*300)        \n",
    "# data.to_csv(\"data_abs{}.csv\".format(str(n_step)),sep=',', index=False, header=True)\n",
    "# # # ============ absolute time step\n",
    "\n",
    "\n",
    "# data fusion\n",
    "name = \"data_rlt10.csv\" ## chosen preprocessing method: relative time step, n_step = 10\n",
    "data = pd.read_csv(name)\n",
    "data['id'] = data['id'].astype(np.int64)\n",
    "\n",
    "data2 = pd.DataFrame(columns=['no_aug']+list(data.columns),data=np.nan*np.zeros((300*n_step*n_aug,1+len(data.columns))))\n",
    "for i,g in data.groupby('id'):\n",
    "    value = []\n",
    "    for j in range(n_aug):\n",
    "        nonnan = (~np.isnan(g[data.columns])).astype(np.int32)\n",
    "        noise = np.random.rand(n_step,len(data.columns))*noise_ratio# add noise\n",
    "        choices = np.random.choice([np.nan,1],n_step*len(data.columns),p=[dropout,1-dropout]).reshape(n_step,len(data.columns))# random dropout\n",
    "        value.append((g[data.columns].values+noise)*choices*nonnan)\n",
    "    aug = np.hstack([[au]*n_step for au in range(n_aug)]).reshape(-1,1)\n",
    "    value = np.hstack((aug,np.vstack(value)))\n",
    "    data2[i*(n_step*n_aug):(i+1)*(n_step*n_aug)] = value  \n",
    "    \n",
    "data2[f_demo+f_others] = np.array([data[f_demo+f_others].groupby('id').head(1).values[i//(n_step*n_aug)] for i in range(data2.shape[0])])\n",
    "data2['time'] = np.array(range(n_step)*300*n_aug)\n",
    "data2.to_csv(\"aug_\"+name,sep=',', index=False, header=True)\n",
    "\n",
    "# minmax scaling\n",
    "minmax_features = f_labs + f_meds + f_demo\n",
    "for f in minmax_features:\n",
    "    mx, mn = data2[~np.isnan(data2[f])][f].max(),data2[~np.isnan(data2[f])][f].min()\n",
    "    print(f,mx,mn)\n",
    "    data2[f] = (data2[f] - mn)/(mx - mn) \n",
    "data2 = data2.fillna(0)\n",
    "print(data2.describe())\n",
    "data2.to_csv(\"final.csv\",sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## KNN input\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# data = pd.read_csv(\"final.csv\")\n",
    "# feature_names = f_demo\n",
    "\n",
    "# features = np.array([g[feature_names].head(1).values.reshape(-1) for _,g in data.groupby('id')])\n",
    "# labels = np.array([g['Stage_Progress'].head(1).values[0] for _,g in data.groupby('id')])\n",
    "# train_test_split = np.load(\"train_test_split.npy\") \n",
    "# train_x = features[train_test_split]\n",
    "# train_y = labels[train_test_split]\n",
    "# test_x = features[~train_test_split]\n",
    "# test_y = labels[~train_test_split]\n",
    "\n",
    "#============= optional: generate model complexity curve for KNN model\n",
    "# dic = {\"test_acc\":[],\"train_acc\":[],\"test_auc\":[],\"train_auc\":[]} \n",
    "# limit=50\n",
    "# for i in range(1,limit):\n",
    "#     knn = KNeighborsClassifier(i)\n",
    "#     pred_test = knn.fit(train_x,train_y).predict_proba(test_x)\n",
    "#     pred_train = knn.fit(train_x,train_y).predict_proba(train_x)\n",
    "    \n",
    "#     train_acc = np.mean(train_y == np.argmax(pred_train,axis=1))\n",
    "#     test_acc = np.mean(test_y == np.argmax(pred_test,axis=1))\n",
    "#     train_auc = roc_auc_score(train_y,pred_train[:,1])\n",
    "#     test_auc = roc_auc_score(test_y,pred_test[:,1])   \n",
    "#     dic['train_acc'].append(train_acc)\n",
    "#     dic['test_acc'].append(test_acc)\n",
    "#     dic['train_auc'].append(train_auc)\n",
    "#     dic['test_auc'].append(test_auc)   \n",
    "    \n",
    "# plt.style.use('default')\n",
    "# plt.figure(figsize=(5,5))\n",
    "# plt.plot(range(1,limit),dic['train_acc'],label='training')\n",
    "# plt.plot(range(1,limit),dic['test_acc'],label='testing')\n",
    "# plt.xlabel(\"Number of Neighbors\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(5,5))\n",
    "# plt.plot(range(1,limit),dic['train_auc'],label='training')\n",
    "# plt.plot(range(1,limit),dic['test_auc'],label='testing')\n",
    "# plt.xlabel(\"Number of Neighbors\")\n",
    "# plt.ylabel(\"AUC score\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "#============= optional: generate predictions from selected KNN model\n",
    "# knn = KNeighborsClassifier(7)\n",
    "# pred_train = knn.fit(train_x,train_y).predict_proba(train_x)\n",
    "# pred_test = knn.fit(train_x,train_y).predict_proba(test_x)\n",
    "# np.save(\"knn_train.npy\",pred_train[:,1])\n",
    "# np.save(\"knn_test.npy\",pred_test[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('n_step', 10)\n",
      "('no of features', 3)\n",
      "('feature_names', ['heart', 'daily_dosage', 'duration'])\n",
      "('train_x', (239, 10, 3))\n",
      "('train_y', (239, 2))\n",
      "('test_x', (61, 10, 3))\n",
      "('test_y', (61, 2))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luwang/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"final.csv\")\n",
    "\n",
    "feature_names =  f_meds # f_labs\n",
    "n_input = len(feature_names)\n",
    "\n",
    "learning_rate = 0.002\n",
    "training_epochs = 1000\n",
    "batch_size = 200\n",
    "n_hidden = 64\n",
    "n_classes = 2\n",
    "alpha = 0.5\n",
    "\n",
    "## split\n",
    "features = np.array([g[feature_names].values for _,g in data.groupby('id')])\n",
    "labels = np.array([g.head(1)['Stage_Progress'].values[0] for _,g in data.groupby('id')])\n",
    "train_test_split = np.load(\"train_test_split.npy\") #np.random.rand(300) < 0.80\n",
    "train_x = features[train_test_split]\n",
    "train_y = labels[train_test_split]\n",
    "test_x = features[~train_test_split]\n",
    "test_y = labels[~train_test_split]\n",
    "\n",
    "train_x = np.vstack(train_x).reshape(-1,n_step,len(feature_names))\n",
    "test_x = np.vstack(test_x).reshape(-1,n_step,len(feature_names))\n",
    "train_y = pd.get_dummies(np.array([[i]*n_aug for i in train_y]).reshape(-1)).values\n",
    "test_y = pd.get_dummies(np.array([[i]*n_aug for i in test_y]).reshape(-1)).values\n",
    "total_batches = (train_x.shape[0]//batch_size)\n",
    "\n",
    "print(\"n_step\",n_step)\n",
    "print(\"no of features\",len(feature_names))\n",
    "print(\"feature_names\",feature_names)\n",
    "print(\"train_x\", train_x.shape)\n",
    "print(\"train_y\", train_y.shape)\n",
    "print(\"test_x\", test_x.shape)\n",
    "print(\"test_y\", test_y.shape)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "# weights init\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "# bias init\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "# model\n",
    "def LSTM(x, weight, bias):\n",
    "    multi_layer_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell.LSTMCell(n_hidden, state_is_tuple=True) for _ in range(2)])\n",
    "    output, state = tf.nn.dynamic_rnn(multi_layer_cell, x, dtype = tf.float32)\n",
    "    output_flattened = tf.reshape(output, [-1, n_hidden])\n",
    "    output_logits = tf.add(tf.matmul(output_flattened,weight),bias)\n",
    "    output_all = tf.nn.sigmoid(output_logits)\n",
    "    output_reshaped = tf.reshape(output_all,[-1,n_step,n_classes])\n",
    "    output_last = tf.gather(tf.transpose(output_reshaped,[1,0,2]), n_step - 1)  \n",
    "    return output_last, output_all\n",
    "x = tf.placeholder(\"float\", [None, n_step, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "y_steps = tf.placeholder(\"float\", [None, n_classes])\n",
    "weight = weight_variable([n_hidden,n_classes])\n",
    "bias = bias_variable([n_classes])\n",
    "y_last, y_all = LSTM(x,weight,bias)\n",
    "# loss function\n",
    "all_steps_cost = -tf.reduce_mean((y_steps * tf.log(y_all))  + (1 - y_steps) * tf.log(1 - y_all))\n",
    "last_step_cost = -tf.reduce_mean((y * tf.log(y_last)) + ((1 - y) * tf.log(1 - y_last)))\n",
    "loss_function = (alpha * all_steps_cost) + ((1 - alpha) * last_step_cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('round', 0)\n",
      "('epoch', 498)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     Stable       0.70      0.88      0.78        40\n",
      "   Progress       0.55      0.29      0.37        21\n",
      "\n",
      "avg / total       0.65      0.67      0.64        61\n",
      "\n",
      "('Accuracy :', 0.67213114754098358, 0.78242677824267781)\n",
      "('AUC Score:', 0.6511904761904761, 0.77958860759493676)\n",
      "('round', 1)\n",
      "('round', 2)\n",
      "('round', 3)\n",
      "('epoch', 296)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     Stable       0.68      1.00      0.81        40\n",
      "   Progress       1.00      0.10      0.17        21\n",
      "\n",
      "avg / total       0.79      0.69      0.59        61\n",
      "\n",
      "('Accuracy :', 0.68852459016393441, 0.67782426778242677)\n",
      "('AUC Score:', 0.66309523809523807, 0.64109968354430391)\n",
      "('epoch', 329)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     Stable       0.67      0.97      0.80        40\n",
      "   Progress       0.67      0.10      0.17        21\n",
      "\n",
      "avg / total       0.67      0.67      0.58        61\n",
      "\n",
      "('Accuracy :', 0.67213114754098358, 0.68619246861924688)\n",
      "('AUC Score:', 0.69047619047619047, 0.65759493670886082)\n",
      "('round', 4)\n",
      "('round', 5)\n",
      "('round', 6)\n",
      "('round', 7)\n",
      "('round', 8)\n",
      "('round', 9)\n"
     ]
    }
   ],
   "source": [
    "# model selection\n",
    "threshold = 0.65\n",
    "for round in range(10):\n",
    "    print(\"round\",round)\n",
    "    save_dic = False\n",
    "    with tf.Session() as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        dic = {\"test_acc\":[],\"train_acc\":[],\"test_auc\":[],\"train_auc\":[]} #record\n",
    "        for epoch in range(training_epochs):\n",
    "            for b in range(total_batches): \n",
    "                choices = np.random.choice(range(len(train_x)),batch_size,replace=True)#random sampling with replacement\n",
    "                batch_x, batch_y = train_x[choices], train_y[choices]\n",
    "                batch_y_steps = np.tile(batch_y,((train_x.shape[1]),1))\n",
    "                _, c = session.run([optimizer, loss_function],feed_dict={x: batch_x, y : batch_y, y_steps: batch_y_steps})   \n",
    "\n",
    "            pred_train_y = session.run(y_last,feed_dict={x:train_x})\n",
    "            pred_test_y = session.run(y_last,feed_dict={x:test_x})\n",
    "            train_acc = np.mean(train_y[:,1] == np.argmax(pred_train_y,axis=1))\n",
    "            test_acc = np.mean(test_y[:,1] == np.argmax(pred_test_y,axis=1))\n",
    "            train_auc = roc_auc_score(train_y,pred_train_y)\n",
    "            test_auc = roc_auc_score(test_y,pred_test_y)\n",
    "            dic['train_acc'].append(train_acc)\n",
    "            dic['test_acc'].append(test_acc)\n",
    "            dic['train_auc'].append(train_auc)\n",
    "            dic['test_auc'].append(test_auc)\n",
    "            \n",
    "            if (test_acc > threshold) and (test_auc > threshold):\n",
    "                report = classification_report(test_y[:,1],np.argmax(pred_test_y,axis=1),target_names=['Stable','Progress'])\n",
    "                save_name = 'round{}_model{}_{}_{}'.format(str(round),str(epoch),str(test_acc),str(test_auc))\n",
    "                print(\"epoch\",epoch)\n",
    "                print(report)\n",
    "                print(\"Accuracy :\",test_acc,train_acc)\n",
    "                print(\"AUC Score:\",test_auc,train_auc)\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session, 'mymodel/'+save_name+\".ckpt\")\n",
    "                with open('mymodel/'+save_name+\"report.pkl\".format(str(round),str(epoch),str(test_acc),str(test_auc)), 'wb') as handle:\n",
    "                    pickle.dump(report, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                save_dic = True\n",
    "                threshold+=0.01\n",
    "        if save_dic:\n",
    "            with open('mymodel/'+save_name+\"dic.pkl\".format(str(round),str(epoch),str(test_acc),str(test_auc)), 'wb') as handle:\n",
    "                pickle.dump(dic, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============= optional: generate model complexity curve for selected LSTM model\n",
    "\n",
    "## with open('mymodel/f_labs_rlt10/round242_model631_0.852459016393_0.836904761905report.pkl', 'rb') as handle:#LSTM11\n",
    "## with open(\"mymodel/f_labs_rlt10/round58_model488_0.819672131148_0.832738095238report.pkl\",'rb') as handle:#LSTM12\n",
    "# with open('mymodel/f_meds_rlt10/2layer/round87_model708_0.737704918033_0.71369047619report.pkl', 'rb') as handle:#LSTM2\n",
    "## with open('mymodel/f_labs_f_meds_rlt10/round46_model234_0.770491803279_0.75119047619report.pkl', 'rb') as handle:#LSTM\n",
    "#     report = pickle.load(handle)\n",
    "#     print(report)\n",
    "## with open('mymodel/f_labs_rlt10/round242_model631_0.852459016393_0.836904761905dic.pkl', 'rb') as handle:#LSTM11\n",
    "## with open(\"mymodel/f_labs_rlt10/round58_model488_0.819672131148_0.832738095238dic.pkl\",'rb') as handle:#LSTM12\n",
    "# with open('mymodel/f_meds_rlt10/2layer/round87_model708_0.737704918033_0.71369047619dic.pkl', 'rb') as handle:#LSTM2\n",
    "## with open('mymodel/f_labs_f_meds_rlt10/round46_model234_0.770491803279_0.75119047619dic.pkl', 'rb') as handle:#LSTM  \n",
    "#     dic=pickle.load(handle)\n",
    "\n",
    "# training_epochs=1000\n",
    "# plt.style.use('default')\n",
    "# plt.figure(figsize=(5,5))\n",
    "# plt.plot(range(training_epochs),dic['train_acc'],label='training')\n",
    "# plt.plot(range(training_epochs),dic['test_acc'],label='testing')\n",
    "# plt.xlabel(\"Epoches\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(5,5))\n",
    "# plt.plot(range(training_epochs),dic['train_auc'],label='training')\n",
    "# plt.plot(range(training_epochs),dic['test_auc'],label='testing')\n",
    "# plt.xlabel(\"Epoches\")\n",
    "# plt.ylabel(\"AUC score\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "#============= optional: generate predictions from selected LSTM model\n",
    "# tf.reset_default_graph()\n",
    "# x = tf.placeholder(\"float\", [None, n_step, n_input])\n",
    "# y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# y_steps = tf.placeholder(\"float\", [None, n_classes])\n",
    "# weight = weight_variable([n_hidden,n_classes])\n",
    "# bias = bias_variable([n_classes])\n",
    "# y_last, y_all = LSTM(x,weight,bias)\n",
    "\n",
    "# with tf.Session() as session:\n",
    "#     saver = tf.train.Saver()\n",
    "# #     saver.restore(session, \"mymodel/f_labs_rlt10/round242_model631_0.852459016393_0.836904761905.ckpt\")#LSTM11\n",
    "# #     saver.restore(session,\"mymodel/f_labs_rlt10/round58_model488_0.819672131148_0.832738095238.ckpt\")#LSTM12\n",
    "#    saver.restore(session, \"mymodel/f_meds_rlt10/round47_model651_0.704918032787_0.697619047619.ckpt\")#LSTM2\n",
    "# #     saver.restore(session, \"mymodel/f_labs_f_meds_rlt10/round46_model234_0.770491803279_0.75119047619.ckpt\")#LSTM \n",
    "#     pred_train = session.run(y_last,feed_dict={x:train_x})\n",
    "#     pred_test = session.run(y_last,feed_dict={x:test_x})\n",
    "    \n",
    "#     train_acc = np.mean(train_y[:,1] == np.argmax(pred_train,axis=1))\n",
    "#     test_acc = np.mean(test_y[:,1] == np.argmax(pred_test,axis=1))\n",
    "#     train_auc = roc_auc_score(train_y,pred_train)\n",
    "#     test_auc = roc_auc_score(test_y,pred_test)\n",
    "#     np.save(\"LSTM2_train.npy\",pred_train[:,1])\n",
    "#     np.save(\"LSTM2_test.npy\",pred_test[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Model: LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Coefficients:', array([[ 0.42025567,  4.66844874,  2.24842019, -0.23361715]]))\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     Stable       0.92      0.82      0.87        40\n",
      "   Progress       0.72      0.86      0.78        21\n",
      "\n",
      "avg / total       0.85      0.84      0.84        61\n",
      "\n",
      "('Accuracy :', 0.83606557377049184)\n",
      "('AUC score:', 0.86071428571428577)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "knn_test = np.load(\"knn_test.npy\")\n",
    "knn_train = np.load(\"knn_train.npy\")\n",
    "LSTM11_test = np.load(\"LSTM11_test.npy\")\n",
    "LSTM11_train = np.load(\"LSTM11_train.npy\")\n",
    "LSTM12_test = np.load(\"LSTM12_test.npy\")\n",
    "LSTM12_train = np.load(\"LSTM12_train.npy\")\n",
    "LSTM2_test = np.load(\"LSTM2_test.npy\")\n",
    "LSTM2_train = np.load(\"LSTM2_train.npy\")\n",
    "LSTM_test = np.load(\"LSTM_test.npy\")\n",
    "LSTM_train = np.load(\"LSTM_train.npy\")\n",
    "\n",
    "train_x = np.vstack((knn_train,LSTM11_train,LSTM12_train,LSTM_train)).T\n",
    "test_x = np.vstack((knn_test,LSTM11_test,LSTM12_test,LSTM_test)).T\n",
    "\n",
    "LR = LogisticRegression()\n",
    "pred_test = LR.fit(train_x,train_y[:,1]).predict_proba(test_x)\n",
    "pred_train = LR.fit(train_x,train_y[:,1]).predict_proba(train_x)\n",
    "print(\"Coefficients:\",LR.coef_)\n",
    "\n",
    "train_acc = np.mean(train_y[:,1] == np.argmax(pred_train,axis=1))\n",
    "test_acc = np.mean(test_y[:,1] == np.argmax(pred_test,axis=1))\n",
    "train_auc = roc_auc_score(train_y,pred_train)\n",
    "test_auc = roc_auc_score(test_y,pred_test)   \n",
    "report = classification_report(test_y[:,1],np.argmax(pred_test,axis=1),target_names=['Stable','Progress'])\n",
    "print(report)\n",
    "print(\"Accuracy :\",test_acc)\n",
    "print(\"AUC score:\",test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
